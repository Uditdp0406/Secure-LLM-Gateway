# ğŸ›¡ï¸ Secure LLM Gateway

> Production-ready FastAPI-based Secure LLM Gateway for safe, scalable, and observable LLM access in enterprise environments.

---

## ğŸ“Œ Overview

Secure LLM Gateway is a middleware layer built to safely expose Large Language Models (LLMs) in production systems.

Instead of directly calling LLM providers (OpenAI, Anthropic, etc.), applications interact with this gateway which enforces:

- ğŸ” Authentication & Authorization  
- ğŸ›¡ Prompt Injection Protection  
- âš¡ Rate Limiting  
- ğŸ“š Retrieval-Augmented Generation (RAG)  
- ğŸ”„ Multi-Model Routing  
- ğŸ“Š Observability & Cost Tracking  

This project demonstrates real-world GenAI backend architecture patterns used in production systems.

---

## ğŸš¨ Why This Exists

Direct LLM API exposure introduces serious risks:

- Prompt injection attacks
- Data exfiltration
- Token abuse and cost spikes
- Lack of governance
- No request monitoring
- No model control layer

This gateway acts as a **security and control plane** between clients and LLM providers.

---

# ğŸ—ï¸ System Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Client    â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ JWT Auth Layerâ”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Rate Limiter  â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Prompt Guards â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   RAG Layer   â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Model Router  â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ LLM Provider (OpenAI /  â”‚
          â”‚ Anthropic / Local LLM)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ” Security Layer

## 1ï¸âƒ£ Authentication & Authorization

- JWT validation
- Role-based access control
- API key verification
- User identity extraction

Prevents unauthorized LLM access.

---

## 2ï¸âƒ£ Prompt Guardrails

Basic protections against:

- Prompt injection attacks
- Jailbreak attempts
- Malicious instruction overrides
- Data exfiltration prompts

Implemented using:

- Regex-based filters
- Keyword blocklist
- Token length validation
- Input schema enforcement

---

## 3ï¸âƒ£ Rate Limiting

Prevents:

- API abuse
- Denial-of-service
- Excessive token usage

Supports:

- Per-user rate limits
- Configurable limits
- Redis-backed distributed rate limiting (optional)

---

# ğŸ“š RAG (Retrieval-Augmented Generation)

Optional retrieval pipeline to reduce hallucination and improve factual grounding.

### Features

- Embedding-based similarity search
- FAISS vector store (or pluggable backend)
- Top-K retrieval
- Metadata filtering
- Context trimming to fit token window
- Re-ranking support (optional)

### Why RAG?

- Improves factual accuracy
- Reduces hallucination
- Enables domain-specific AI
- Provides context-aware responses

---

# ğŸ”„ Model Routing Layer

Supports dynamic routing between:

- OpenAI
- Anthropic
- Local/self-hosted models

Routing can be configured based on:

- User tier (free vs premium)
- Cost optimization
- Fallback strategies
- Model availability
- Request type

---

# ğŸ“Š Observability & Monitoring

Each request logs:

- Request ID
- User ID
- Model used
- Latency
- Token usage (estimated)
- Cost estimation
- Timestamp

Enables:

- Cost tracking
- Performance monitoring
- Usage analytics
- Debugging
- Abuse detection

---

# ğŸ§  Concurrency & Scalability

Built using:

- FastAPI (async support)
- Non-blocking request handling
- Stateless architecture
- Horizontal scalability ready
- Docker-compatible

Supports concurrent multi-user LLM workloads.

---

# ğŸ› ï¸ Tech Stack

- Python 3.10+
- FastAPI
- Pydantic
- FAISS (optional)
- Redis (optional)
- Docker
- OpenAI SDK
- Anthropic SDK

---

# ğŸ“ Project Structure

```
secure-llm-gateway/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â”œâ”€â”€ guardrails.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ router.py
â”‚   â”‚   â”œâ”€â”€ providers.py
â”‚   â”‚   â””â”€â”€ prompt_sanitizer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ reranker.py
â”‚   â”‚   â””â”€â”€ context_builder.py
â”‚   â”‚
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ request.py
â”‚       â””â”€â”€ response.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

Clear separation of concerns:

- API Layer
- Security Layer
- LLM Routing Layer
- Retrieval Layer
- Observability Layer

---

# â–¶ï¸ Running Locally

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/secure-llm-gateway.git
cd secure-llm-gateway
```

## 2ï¸âƒ£ Create `.env` File

```
OPENAI_API_KEY=your_api_key
ANTHROPIC_API_KEY=your_key
JWT_SECRET=your_secret
```

## 3ï¸âƒ£ Run with Docker

```bash
docker-compose up --build
```

Or run manually:

```bash
pip install -r requirements.txt
uvicorn app.main:app --reload
```

---

# ğŸ“Œ Example API Call

```
POST /v1/chat
Authorization: Bearer <JWT_TOKEN>
Content-Type: application/json

{
  "model": "gpt-4",
  "prompt": "Explain vector databases in simple terms.",
  "use_rag": true
}
```

---

# ğŸ¯ Use Cases

- Enterprise AI systems
- Secure internal AI assistants
- Production RAG systems
- SaaS AI platforms
- Multi-tenant AI infrastructure
- Agent-based systems

---

# ğŸ”® Future Improvements

- Multi-tenant isolation
- Streaming responses
- LLM fallback mechanism
- Advanced semantic injection detection
- OpenTelemetry tracing
- Kubernetes deployment
- Cost dashboard
- Prompt caching

---

# ğŸ“„ License

MIT License

---

# ğŸ‘¨â€ğŸ’» Author

Built as a production-style GenAI backend system focusing on:

- Security
- Scalability
- Reliability
- Observability
- Clean architecture design
